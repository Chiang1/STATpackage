---
title: "Project 3: STAT302package Tutorial"
author: "Chunl Chiang"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{STAT302package Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

This package is a compilation of functions that run some statistical inferences and predictions that we have learned over the course of this class. The STAT302package includes functions my_t.test, my_lm and my_knn_cv, as well as two sets of data my_penguins and my_gapminder from the gapminder and palmerpenguins packages. For statistical inference, we have the function my_t.test that performs a one sample t-test and my_lm fits a linear model. For statistical prediction, other than my_lm, it has the my_knn_cv function that predicts an output class using covariates.

Install STAT302package using:
```{r, eval = FALSE}
devtools::install_github("Chiang1/STAT302package")
```

Loading packages needed as well as the data
```{r, message=FALSE}
library(STAT302package)
library(dplyr)
library(ggplot2)
data("my_gapminder")
data("my_penguins") 
```

# my_t.test Tutorial 

```{r}
# Creating a vector of lifeExp from my_gapminder for t-tests
life_exp <- my_gapminder %>% pull(lifeExp)
```

### Demonstrating a test of the hypothesis $$H_0 : \mu = 60,$$ $$H_a : \mu \neq 60.$$ 

```{r}
my_t.test(x = life_exp, alternative = "two.sided", mu = 60)
```

With a p-value cut-off of $\alpha = 0.05$ from the two.sided test we can conclude that our results are not statistically significant. Therefore, we cannot reject the null hypothesis because p_val = 0.093 which is greater than 0.05. From this we cannot say that the mean of lifeExp is not equal to 60. 

### Demonstrating a test of the hypothesis $$H_0 : \mu = 60,$$ $$H_a : \mu < 60.$$ 

```{r}
my_t.test(x = life_exp, alternative = "less", mu = 60)
```

With a p-value cut-off of $\alpha = 0.05$ from the one sided test with $H_a : \mu < 60$ we can conclude that our results are statistically significant. Therefore, we can reject the null hypothesis and have evidence to support our alternative hypothesis because the p_val = 0  < $\mu = 0.05$.

### Demonstrating a test of the hypothesis $$H_0 : \mu = 60,$$ $$H_a : \mu > 60.$$ 

```{r}
my_t.test(x = life_exp, alternative = "greater", mu = 60)
```

With a p-value cut-off of $\alpha = 0.05$ from the one sided test with $H_a : \mu > 60$ we can conclude that our results are not statistically significant. Therefore, we cannot reject the null hypothesis becuase p_val = 1 which is greater than 0.05.From this we cannot say that the mean of lifeExp greater than 60. 

# my_lm Tutorial

Demonstrating a regression of lifeExp as the response variable and gdpPercap and continent as the explanatory variables. 

```{r}
# Fitting a linear model 

life_reg <- my_lm(lifeExp ~ gdpPercap + continent, data = my_gapminder)
life_reg

```

This table shows us the estimate, standard error, t-value and p-value of each coefficient. Let us interpret the gdpPercap coefficient. The estimate means that an increase of 1 in the gdpPercap means the lifeExp increases by `r life_reg[2, 1]`. The standard error is `r life_reg[2, 2]` which represents the average distance from the observed values of lifeExp from the regression line. To understand the last two values we need to understand the hypothesis test associated with the gdpPercap coefficient ($\beta$) which is: $$H_0 : \beta = 0$$ $$H_a : \beta \neq 0$$
We test this hypothesis with a t-test, like in the previous section, in order to get the last two column values. This will tell us if we can reject the null hypothesis which says that the coefficient has no impact on lifeExp. The t-value is `r life_reg[2, 3]` which is $\frac{estimate - 0}{std. error}$, with which we find the p-value, `r life_reg[2, 4]`. With a significance value of $\alpha = 0.05$, we can reject the null hypothesis which says gdpPercap has no impact on lifeExp, because `r life_reg[2, 4]` $< 0.05$. 

```{r}
# creating actual vs fitted plot data
# isolating X
mat_X <- model.matrix(lifeExp ~ gdpPercap + continent, data = my_gapminder)
# isolating betas
mat_b <- life_reg %>% pull(Estimate)
# creating fitted values
mat_fitted <- mat_X %*% mat_b
# creating data frame with continent, fitted, and actual values
act_vs_fit <- data.frame("Continent" = my_gapminder$continent,
                         "Fitted" = mat_fitted,
                         "Actual" = my_gapminder$lifeExp)

```

```{r, fig.height = 5, fig.width=8}
# creating actual vs fitted plot
ggplot(act_vs_fit, aes(x = Fitted, y = Actual, color = Continent)) +
  geom_point() +
  # resizing font 
  theme_bw(base_size = 15) +
  # changing labels
  labs(title = "Actual vs. Fitted") +
  # adjusting placement
  theme(plot.title =
          element_text(hjust = 0.5),
         plot.caption =
           element_text(hjust = 0))

```

Looking at this graph of the actual lifeExp vs fitting lifeExp, we can see that the points are all clustered by their continent. Taking a look at the continent of Africa we see that fitted values are around 50 while the actual values have a broader range. This is the same among other continents, where the range of actual values is much greater than of the fitted values. This tells us that the variable, continent, plays a big role in the model. Since the variables in this model are only continents and gdpPercap, we can conclude that the variation in fitted values by continent is due to gdpPercap. For example, Europe has a different range in fitted values than Asia because of gdgPercap. 

# my_knn_cv Tutorial

In this tutorial, we will predict output class species using covariates bill_length_mm, flipper_length_mm, and body_mass_g. We will use a 5-fold cross validation, so k_cv $= 5$. We will iterate from k_nn $= 1, ..., 10$. 

```{r}
# cleaning data 
data_clean <- na.omit(my_penguins %>% select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g))

# filtering for training data
p_train <- data_clean %>% select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

```

```{r}
# errors(): function that creates data frame with training misclasssification errors 
# and CV missclassification rate for multiple iterations of k_nn for data above
# input: the highest k_nn = j
# output: data frame
errors <- function(j) {
  # initializing data frames
  cv_err_df <- data.frame()
  train_err_df <- data.frame()
  # iteration of k_nn 1 to j
  for (i in 1:j) {
    ## running my_knn_cv
    ## Here is where we used my_knn_cv to predict class and find CV_MSE
    knn <- my_knn_cv(p_train, data_clean$species, i, 5)
    # Adding cv_error to table
    cv_err_df <- rbind(cv_err_df, i = knn[["cv_err"]])
    # finding training error 
    train_err <- mean(ifelse(knn[["class"]] == data_clean$species, 0, 1))
    # adding training error to data frame
    train_err_df <- rbind(train_err_df, i = train_err)
  }
  # cbind tables
  error_tab <- cbind(cv_err_df, train_err_df)
  colnames(error_tab) <- c("CV misclassification rate", "training misclassification rate")
  rownames(error_tab) <- c(1:j)
  return(error_tab)
}

```

```{r}
# iterating for k_nn 1 to 10
errors(10)

```

The function my_knn_cv uses k-fold cross validation, and here we used a 5-fold cross validation. Cross validation is when we split data in k folds, use all but one fold as the training data and the one fold as the test data; in this one fold we find the mean squared error (MSE). We repeat this until all folds have been the test data, then we calculate the mean of all the MSEs to find our cross validation estimate (CV). We do this in order to properly evaluate the performance of estimator on data that was not used to train it. The table above shows the misclassification rates for k_nn 1 to 10. Based on the training misclassification rates I would chose the model where k_nn equals 1, and would chose the same based on CV misclassification rate. When the model is fitted using 1-nearest neighbors both the CV and training misclassification rates are the lowest. In practice, we choose the model that has the lower CV error rate and then re-fit the model with the whole data. Therefore, in this case we choose the model with k_nn $= 1$.  



